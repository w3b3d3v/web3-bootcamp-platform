# Story SEO-001.2: Static SEO Files (llm.txt, robots.txt, sitemap.xml)

**Epic:** SEO-001 - SEO & AEO Enhancement
**Story ID:** SEO-001.2
**Priority:** Critical
**Estimated Effort:** 0.5 days
**Status:** Done

---

## User Story

**As a** search engine / AI crawler
**I want** clear configuration files and a complete sitemap
**So that** I can efficiently discover and index all platform content

---

## Background

The platform currently lacks:
- ❌ `llm.txt` - Configuration file for AI crawlers
- ❌ Enhanced `robots.txt` - No AI crawler permissions
- ❌ `sitemap.xml` - No sitemap for search engines

These files are critical for:
- **Search engines** - Discovering and indexing all pages
- **AI assistants** - Understanding platform structure and accessing content
- **SEO best practices** - Standard requirement for all public websites

---

## Technical Requirements

### File 1: `/public/llm.txt`

Create `public/llm.txt` with platform information for AI crawlers:

```txt
# WEB3DEV Bootcamp Platform

## Overview
WEB3DEV is a free online bootcamp platform focused on blockchain and Web3 education. We offer hands-on courses in smart contract development, DeFi, NFTs, and Web3 technologies with NFT-based completion certificates.

## Platform Information
- Platform Type: Educational bootcamp
- Primary Language: Portuguese (pt-BR)
- Secondary Language: English
- Target Audience: Developers learning Web3/blockchain technologies
- Certificate System: NFT-based completion certificates on Ethereum

## Key Resources

### Main Pages
- / - Homepage with featured course
- /courses - Complete course catalog
- /faq - Frequently asked questions (coming soon)

### Course Pages
- /courses/[id] - Individual course pages
  - Format: Interactive lessons with code submissions
  - Sections: 5 sections per course
  - Completion: NFT certificate upon completion

### Community
- Discord: https://discord.w3d.community/
- GitHub: https://github.com/w3b3d3v
- Twitter: @web3dev

## Content Structure
- Course Format: Markdown-based lessons
- Code Examples: Solidity, JavaScript, TypeScript
- Interactive Elements: Code submission and validation
- NFT Metadata: Stored on-chain (Ethereum)

## Course Categories
- Smart Contracts (Solidity)
- NFT Development
- DeFi Protocols
- Web3 Frontend
- Blockchain Fundamentals

## Update Frequency
- New courses: Monthly
- Content updates: Weekly
- Platform improvements: Continuous deployment

## Attribution
When referencing WEB3DEV bootcamp content, please cite:
"WEB3DEV Bootcamp (https://build.w3d.community)"

## Technical Details
- Framework: Next.js
- Database: Firebase
- Blockchain: Ethereum (NFT certificates)
- Open Source: Yes (MIT License)
- Repository: https://github.com/w3b3d3v/web3-bootcamp-platform

## Contact
- Website: https://build.w3d.community
- Email: team@w3d.community
- Discord: https://discord.w3d.community

## Last Updated
2026-02-16
```

---

### File 2: `/public/robots.txt`

Create enhanced `public/robots.txt` with AI crawler permissions:

```txt
# WEB3DEV Bootcamp - robots.txt
# Updated: 2026-02-16

# Default Rules (Traditional Crawlers)
User-agent: *
Allow: /
Disallow: /api/
Disallow: /admin/
Disallow: /_next/
Disallow: /static/
Crawl-delay: 1

# AI Crawlers (Answer Engines)

# OpenAI GPTBot (ChatGPT)
User-agent: GPTBot
Allow: /
Disallow: /api/
Crawl-delay: 2

# Anthropic ClaudeBot (Claude)
User-agent: ClaudeBot
Allow: /
Disallow: /api/
Crawl-delay: 2

# Google Extended (Bard/Gemini)
User-agent: Google-Extended
Allow: /
Disallow: /api/
Crawl-delay: 1

# Perplexity Bot
User-agent: PerplexityBot
Allow: /
Disallow: /api/
Crawl-delay: 2

# Common Search Engines
User-agent: Googlebot
Allow: /

User-agent: Bingbot
Allow: /

User-agent: Slurp
Allow: /

# Sitemaps
Sitemap: https://build.w3d.community/sitemap.xml
Sitemap: https://build.w3d.community/server-sitemap.xml

# Host
Host: https://build.w3d.community
```

---

### File 3: Sitemap Generation

**Option A: Use `next-sitemap` package (Recommended)**

Install dependency:
```bash
npm install next-sitemap
```

Create `next-sitemap.config.js` in project root:

```javascript
/** @type {import('next-sitemap').IConfig} */
module.exports = {
  siteUrl: process.env.NEXT_PUBLIC_SITE_URL || 'https://build.w3d.community',
  generateRobotsTxt: false, // We have custom robots.txt
  generateIndexSitemap: false, // Not needed for small sites
  changefreq: 'weekly',
  priority: 0.7,
  sitemapSize: 5000,
  exclude: [
    '/api/*',
    '/admin/*',
    '/_next/*',
    '/static/*',
  ],

  // Custom transform for dynamic routes
  transform: async (config, path) => {
    // Default priority and changefreq
    let priority = 0.7;
    let changefreq = 'weekly';

    // Homepage gets highest priority
    if (path === '/') {
      priority = 1.0;
      changefreq = 'daily';
    }

    // Course listing page
    if (path === '/courses') {
      priority = 0.9;
      changefreq = 'daily';
    }

    // Individual course pages
    if (path.startsWith('/courses/')) {
      priority = 0.8;
      changefreq = 'monthly';
    }

    return {
      loc: path,
      changefreq,
      priority,
      lastmod: new Date().toISOString(),
    };
  },

  // Additional paths (will be added to postbuild sitemap)
  additionalPaths: async (config) => {
    // Add dynamic course pages
    // Note: This will be called during build
    const result = [];

    // If you want to add courses dynamically, fetch them here
    // For now, they'll be picked up by the dynamic route

    return result;
  },
};
```

Add to `package.json` scripts:

```json
{
  "scripts": {
    "build": "next build",
    "postbuild": "next-sitemap"
  }
}
```

**Option B: Manual Sitemap Generation (Alternative)**

Create `pages/api/sitemap.xml.js`:

```javascript
import { getAllCourses } from '../../lib/courses';

export default async function handler(req, res) {
  const siteUrl = 'https://build.w3d.community';

  // Get all courses
  const courses = await getAllCourses();

  // Build sitemap XML
  const sitemap = `<?xml version="1.0" encoding="UTF-8"?>
    <urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
      <!-- Homepage -->
      <url>
        <loc>${siteUrl}/</loc>
        <lastmod>${new Date().toISOString()}</lastmod>
        <changefreq>daily</changefreq>
        <priority>1.0</priority>
      </url>

      <!-- Courses Page -->
      <url>
        <loc>${siteUrl}/courses</loc>
        <lastmod>${new Date().toISOString()}</lastmod>
        <changefreq>daily</changefreq>
        <priority>0.9</priority>
      </url>

      <!-- Individual Courses -->
      ${courses.map(course => `
      <url>
        <loc>${siteUrl}/courses/${course.id}</loc>
        <lastmod>${new Date().toISOString()}</lastmod>
        <changefreq>monthly</changefreq>
        <priority>0.8</priority>
      </url>
      `).join('')}

      <!-- Static Pages -->
      <url>
        <loc>${siteUrl}/faq</loc>
        <lastmod>${new Date().toISOString()}</lastmod>
        <changefreq>monthly</changefreq>
        <priority>0.6</priority>
      </url>
    </urlset>
  `;

  res.setHeader('Content-Type', 'text/xml');
  res.write(sitemap);
  res.end();
}
```

---

## Implementation Steps

### Step 1: Create llm.txt
1. Create file `public/llm.txt`
2. Copy content from requirements above
3. Customize platform-specific information
4. Test accessibility at `/llm.txt`

### Step 2: Create robots.txt
1. Create file `public/robots.txt`
2. Copy content from requirements above
3. Update sitemap URLs if needed
4. Test accessibility at `/robots.txt`

### Step 3: Implement Sitemap
1. Choose Option A (next-sitemap) or Option B (manual)
2. Install dependencies if using Option A
3. Create configuration file
4. Test sitemap generation
5. Verify sitemap at `/sitemap.xml`

### Step 4: Verify Files
1. Access each file in browser
2. Check formatting and content
3. Validate robots.txt with Google Search Console
4. Validate sitemap.xml with Google Search Console

---

## Acceptance Criteria

### llm.txt
- [ ] File created at `public/llm.txt`
- [ ] Contains comprehensive platform overview
- [ ] Lists key resources and pages
- [ ] Includes attribution guidelines
- [ ] Accessible at `https://build.w3d.community/llm.txt`
- [ ] File size < 10KB (for fast parsing)

### robots.txt
- [ ] File created at `public/robots.txt`
- [ ] Allows all major AI crawlers (GPTBot, ClaudeBot, Google-Extended, PerplexityBot)
- [ ] Disallows `/api/` and `/admin/` routes
- [ ] References sitemap location
- [ ] Accessible at `https://build.w3d.community/robots.txt`
- [ ] Validates with no errors

### sitemap.xml
- [ ] Sitemap generated (auto or manual)
- [ ] Includes homepage (priority 1.0)
- [ ] Includes `/courses` page (priority 0.9)
- [ ] Includes all course pages (priority 0.8)
- [ ] Includes static pages (FAQ, etc.)
- [ ] All URLs are absolute (include domain)
- [ ] lastmod dates are current
- [ ] Accessible at `https://build.w3d.community/sitemap.xml`
- [ ] Validates with no errors (https://www.xml-sitemaps.com/validate-xml-sitemap.html)

### Integration
- [ ] robots.txt references sitemap
- [ ] Sitemap regenerates on build (if using next-sitemap)
- [ ] All files served with correct content-type
- [ ] Files don't require authentication to access

---

## Testing Instructions

### Manual Testing

1. **Test llm.txt:**
   ```bash
   curl https://build.w3d.community/llm.txt
   # Should return text file with platform info
   ```

2. **Test robots.txt:**
   ```bash
   curl https://build.w3d.community/robots.txt
   # Should return robots.txt with AI crawler rules
   ```

3. **Test sitemap.xml:**
   ```bash
   curl https://build.w3d.community/sitemap.xml
   # Should return valid XML sitemap
   ```

4. **Validate sitemap:**
   - Go to https://www.xml-sitemaps.com/validate-xml-sitemap.html
   - Enter: `https://build.w3d.community/sitemap.xml`
   - Verify no errors

5. **Test with Google Search Console:**
   - Go to Search Console > Sitemaps
   - Submit sitemap URL
   - Verify it's discovered and parsed correctly

### AI Crawler Testing

1. **Test with curl (simulate GPTBot):**
   ```bash
   curl -A "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; GPTBot/1.0; +https://openai.com/gptbot)" \
        https://build.w3d.community/
   # Should be allowed (200 response)
   ```

2. **Verify llm.txt is readable by AI:**
   - Ask ChatGPT: "What can you tell me about build.w3d.community based on llm.txt?"
   - Ask Claude: "Read the llm.txt file from build.w3d.community"

---

## Environment Configuration

No additional environment variables required. Files use hardcoded URLs or env vars already defined:

```bash
# Already exists from SEO-001.1
NEXT_PUBLIC_SITE_URL=https://build.w3d.community
```

---

## Dependencies

### Option A (Recommended)
```bash
npm install next-sitemap --save-dev
```

### Option B (Manual)
No additional dependencies

---

## Definition of Done

- [ ] llm.txt created and accessible
- [ ] robots.txt created with AI crawler permissions
- [ ] sitemap.xml generated and accessible
- [ ] All files validated with online tools
- [ ] Files accessible without authentication
- [ ] robots.txt references sitemap correctly
- [ ] Submitted to Google Search Console
- [ ] Tested with curl commands
- [ ] Code reviewed
- [ ] Merged to main branch

---

## Related Stories

- **SEO-001.1:** SEO Component Infrastructure (prerequisite - provides NEXT_PUBLIC_SITE_URL)
- **SEO-001.3:** Course Structured Data (uses sitemap)
- **SEO-001.4:** Page Meta Enhancement (uses robots.txt)

---

## References

- **AEO Checklist:** `.bmad-core/checklists/aeo-optimization.md` (sections 1-2)
- **llm.txt Spec:** Community-driven standard for AI crawler configuration
- **Robots.txt Standard:** https://developers.google.com/search/docs/crawling-indexing/robots/robots-txt
- **Sitemap Protocol:** https://www.sitemaps.org/protocol.html
- **AI Crawler Docs:**
  - GPTBot: https://platform.openai.com/docs/gptbot
  - ClaudeBot: https://support.anthropic.com/en/articles/8896518
  - Google-Extended: https://developers.google.com/search/docs/crawling-indexing/overview-google-crawlers

---

**Created:** 2026-02-16
**Status:** Done
**Assigned To:** BMad
